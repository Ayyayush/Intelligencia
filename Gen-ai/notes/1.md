# ğŸ“˜ Lecture 1  Introduction Notes â€”  CampusX Generative AI (GenAI)

---

## ğŸ¤– What is Generative AI?

**Generative AI** ek type ki Artificial Intelligence hai jo **naya content create** karti hai by learning patterns from existing data.

### ğŸ§¾ Content Types
- âœï¸ Text  
- ğŸ–¼ï¸ Images  
- ğŸµ Music  
- ğŸ’» Code  

ğŸ‘‰ Pehle AI zyada tar **decision-making** tak limited thi,  
GenAI ke aane ke baad **creative tasks** bhi possible ho gaye.

---

## ğŸ•°ï¸ Background: Before Generative AI

### 1ï¸âƒ£ Symbolic AI (1980s)
- Rule-based systems  
- Logic & predefined rules  
- Example: IFâ€“ELSE systems  

### 2ï¸âƒ£ Fuzzy AI
- Uncertainty aur imprecision handle karti thi  
- Partial truth values (0â€“1 ke beech)

### âŒ Limitations of Early AI
- Writing, images, music jaise creative kaam nahi kar paati thi  
- Focus sirf decision systems par tha  

ğŸ‘‰ **Generative AI ne ye limitation tod di**

---

## ğŸ§¬ AI Evolution Path


Artificial Intelligence (AI)
        â†“
Machine Learning (ML)
        â†“
Deep Learning (DL)
        â†“
Generative AI (GenAI)





## ğŸ§  GenAI Mein Kya Kar Sakte Hain?

GenAI mein mainly **2 approaches** hoti hain:

### 1ï¸âƒ£ Use Foundation Models
- Prompt engineering  
- APIs ka use karna  

### 2ï¸âƒ£ Build / Customize Foundation Models
- Fine-tuning  
- RLHF (Reinforcement Learning with Human Feedback)  
- Training pipelines  

---

## ğŸ‘¥ User Side vs Builder Side (Very Important)

| Term | Side |
|----|----|
| Prompt Engineering | User |
| RAG (Retrieval Augmented Generation) | User |
| Fine-tuning | User + Builder |
| RLHF | Builder |
| AI Agents | Builder |
| Vector Databases | Builder |

ğŸ‘‰ Koi bhi new term mile â†’ pehle check karo **user side** ya **builder side**

---




## ğŸŒ Impact Areas of Generative AI

- ğŸ§‘â€ğŸ’¼ Customer Support (chatbots, ticket handling)  
- âœï¸ Content Creation  
- ğŸ“ Education (personal tutors)  
- ğŸ’» Software Development (code generation)  

---

## â“ Is Generative AI Successful?

### Internet vs Crypto Comparison

### âœ” Does it solve real-world problems?
- Banking  
- Shopping  
- Support tickets  
- Personal tutors  

ğŸ‘‰ **Yes**

### âœ” Daily Life Impact?
- World economics pe impact  
- Easy to use  
- New jobs create kar raha hai  

ğŸ“ˆ Abhi full potential tak nahi pahuncha â€” **but very close**

---



## ğŸ“Œ Prerequisites

- Machine Learning fundamentals  
- Deep Learning fundamentals  
- Deep Learning frameworks  
- PyTorch **or** TensorFlow  

---







## ğŸš€ Curriculum Starts With

## Builder perspdective (Using Generative AI)

### ğŸ”¹ Transformer Architecture

Transformer architecture GenAI ka **core foundation** hai.  
Almost saare modern LLMs (GPT, Gemini, Claude) isi pe based hote hain.

---

## ğŸ§© Types of Transformers

### 1ï¸âƒ£ Encoder-Only
- Input ko samajhne / represent karne ke liye use hota hai
- Best for **understanding tasks**

**Examples:**
- BERT  
- RoBERTa  

**Use cases:**
- Classification  
- NER  
- Sentiment Analysis  

---

### 2ï¸âƒ£ Decoder-Only
- Sirf **text generate** karne ke liye optimized
- Next-token prediction pe kaam karta hai

**Examples:**
- GPT series  

**Use cases:**
- Text generation  
- Chatbots  
- Code generation  

---

### #ï¸âƒ£ Encoderâ€“Decoder (Seq2Seq)
- Encoder input samajhta hai  
- Decoder output generate karta hai  

**Examples:**
- T5  
- BART  

**Use cases:**
- Translation  
- Summarization  
- Question Answering  

---

### 4ï¸âƒ£ GPT Architecture
- Decoder-only transformer
- Autoregressive model
- Large-scale foundation model

ğŸ‘‰ **Most GenAI applications isi category mein aati hain**

---

## ğŸ—ï¸ After Transformers â†’ Pretraining

### ğŸ”¹ Pretraining Kya Hota Hai?
Model ko **large-scale generic data** par train karna taaki wo language samajh sake.

### Topics Covered:
- **Training objectives** (next token prediction, masked LM, etc.)
- **Tokenization**
  - Word-based
  - Subword (BPE, WordPiece)
- **Training strategies**
  - Batch size
  - Learning rate
- **Handling challenges**
  - Long context
  - Data noise
  - Bias

---

## âš™ï¸ Optimization

Foundation models ka size bahut bada hota hai, isliye optimization zaroori hoti hai.

### ğŸ”¹ Areas of Optimization

#### 1ï¸âƒ£ Training Optimization
- Faster training
- Efficient GPU usage

#### 2ï¸âƒ£ Model Compression
- Model size chhota karna
- Techniques:
  - Quantization
  - Pruning
  - Distillation

#### #ï¸âƒ£ Inference Optimization
- Fast response time
- Low latency
- Production-ready serving

---

## ğŸ¯ Fine-Tuning

Pretrained model ko **specific task ke liye customize** karna.

### Types of Fine-Tuning:

#### ğŸ”¹ Task-Specific Fine-Tuning
- Classification
- QA
- Domain-specific tasks

#### ğŸ”¹ RLHF (Reinforcement Learning with Human Feedback)
- Human feedback se model behavior improve karna
- Alignment ke liye use hota hai

#### ğŸ”¹ Instruction Tuning
- Model ko instructions follow karna sikhana
- Chat-style behavior

#### ğŸ”¹ Continual Pretraining
- New data pe dubara train karna
- Knowledge update karna

---

## ğŸ“Š Evaluation

Model ki quality measure karne ke liye:
- Accuracy
- BLEU / ROUGE
- Human evaluation
- Bias & safety checks

ğŸ‘‰ Ye decide karta hai model **usable hai ya nahi**

---

## ğŸš€ Deployment

Final stage jahan model:
- API ban jaata hai
- Production mein jata hai

Includes:
- Model hosting
- Scaling
- Monitoring
- Cost optimization

---

## ğŸ§  Big Picture Summary
Transformers
   â†“
Pretraining
   â†“
Optimization
   â†“
Fine-tuning
   â†“
Evaluation
   â†“
Deployment






## User Perspective (Using Generative AI)

From the **user / application perspective**, focus is on **using existing models** to build real-world applications rather than training models from scratch.

---

### Building Basic LLM Apps
- Simple applications using Large Language Models  
- Examples:
  - Chatbots  
  - Q&A systems  
  - Text summarizers  

---

### Open Source vs Closed Source LLMs

#### Open Source LLMs
- Model weights are publicly available  
- More control and customization  
- Can run locally or on own infrastructure  

Examples:
- LLaMA
- Mistral
- Falcon

#### Closed Source LLMs
- Model weights are not public  
- Access through APIs  
- Easier to use, less control  

Examples:
- GPT
- Gemini
- Claude

---

### Using LLM APIs
- Models are accessed via APIs  
- No need to manage infrastructure  
- Faster development  

Common use cases:
- Text generation  
- Chat interfaces  
- Code assistance  

---

### LangChain
- Framework for building LLM-powered applications  
- Helps in:
  - Prompt management  
  - Chains and workflows  
  - Tool integration  

---

### Hugging Face
- Platform for:
  - Models  
  - Datasets  
  - Transformers library  
- Used for:
  - Running open-source models  
  - Experimentation and fine-tuning  

---

### Ollama
- Tool to run LLMs **locally**  
- Useful for:
  - Local development  
  - Privacy-focused applications  

---

## Prompt Engineering
- Designing effective prompts to get better outputs  
- Involves:
  - Clear instructions  
  - Context setting  
  - Examples  

---

## RAG (Retrieval Augmented Generation)
- Combines:
  - LLMs  
  - External data sources  

Flow:
- Retrieve relevant data  
- Pass it to the LLM  
- Generate more accurate answers  

Used when:
- Data is dynamic  
- Model does not have required knowledge  

---

## Fine-Tuning (User-Level)
- Fine-tuning here is **simpler**
- Mostly involves:
  - Adapting existing models  
  - Small datasets  
- Used to:
  - Improve domain-specific performance  

---

## Agents
- Very large and important field  
- AI agents can:
  - Take decisions  
  - Use tools  
  - Perform multi-step tasks  

Examples:
- Autonomous workflows  
- Tool-using agents  

---

## LLM Ops
- Operational side of LLM applications  
- Includes:
  - Deployment  
  - Monitoring  
  - Logging  
  - Cost optimization  

Similar to:
- MLOps but focused on LLMs  

---

## Miscellaneous
- Other supporting concepts  
- Tools and practices that help in:
  - Building  
  - Scaling  
  - Maintaining GenAI applications  
